_target_: hptr_modules.pl_modules.waymo_motion.WaymoMotion

time_step_current: 10
time_step_end: 21
n_video_batch: 3
interactive_challenge: False
inference_cache_map: False
inference_repeat_n: 1

train_metric:
  _target_: hptr_modules.models.metrics.nll.NllMetrics
  winner_takes_all: hard1 # none, or (joint) + hard + (1-6), or cmd
  l_pos: nll_torch # nll_torch, nll_mtr, huber, l2
  p_rand_train_agent: -1 # 0.2
  n_step_add_train_agent: [-1, 40, 40] # -1 to turn off
  focal_gamma_conf: [0.0, 0.0, 0.0] # 0.0 to turn off
  w_conf: [1.0, 1.0, 1.0] # veh, ped, cyc
  w_pos: [1.0, 1.0, 1.0]
  w_yaw: [1.0, 1.0, 1.0]
  w_vel: [1.0, 1.0, 1.0]
  w_spd: [1.0, 1.0, 1.0]

waymo_metric:
  _target_: hptr_modules.models.metrics.waymo.WaymoMetrics
  n_max_pred_agent: 64

pre_processing:
  scene_centric:
    _target_: hptr_modules.data_modules.scene_centric.SceneCentricPreProcessing
    gt_in_local: True
    mask_invalid: False
  global:
    _target_: hptr_modules.data_modules.sc_global.SceneCentricGlobal
    dropout_p_history: -1
    use_current_tl: True
    add_ohe: True
    pl_aggr: False # True for using our MLP as polyline subnet, False for using VectorNet PointNet as polyline subnet
    pose_pe: # xy_dir, mpa_pl, pe_xy_dir, pe_xy_yaw
      agent: pe_xy_dir
      map: pe_xy_dir
      tl: pe_xy_dir

post_processing:
  to_dict:
    _target_: hptr_modules.data_modules.post_processing.ToDict
    predictions: ${...model.decoder.mlp_head.predictions}
  get_cov_mat:
    _target_: hptr_modules.data_modules.post_processing.GetCovMat
    rho_clamp: 5.0
    std_min: -1.609
    std_max: 5.0
  waymo:
    _target_: hptr_modules.data_modules.waymo_post_processing.WaymoPostProcessing
    k_pred: 1
    use_ade: True
    score_temperature: -1
    mpa_nms_thresh: [2.5, 1.0, 2.0] # veh, ped, cyc
    mtr_nms_thresh: []
    gt_in_local: ${...pre_processing.scene_centric.gt_in_local}

model:
  _target_: hptr_modules.models.sc_global.SceneCentricGlobal
  hidden_dim: 256
  n_tgt_knn: 36
  dist_limit_map: 1500
  dist_limit_tl: 1000
  dist_limit_agent: [1500, 500, 1000]
  decoder_remove_ego_agent: False
  n_decoders: 1
  tf_cfg:
    n_head: 4
    dropout_p: 0.1
    norm_first: True
    bias: True
  intra_class_encoder:
    n_layer_mlp: 3
    mlp_cfg:
      end_layer_activation: True
      use_layernorm: False
      use_batchnorm: False
      dropout_p: null
    n_layer_tf_map: 6
    n_layer_tf_tl: -1
    n_layer_tf_agent: -1
  decoder:
    _target_: hptr_modules.models.sc_global.Decoder
    n_pred: 1
    tf_n_layer: 2
    k_reinforce_tl: 2
    k_reinforce_agent: 4
    k_reinforce_all: -1
    k_reinforce_anchor: 10
    n_latent_query: -1
    latent_query:
      use_agent_type: False
      mode_emb: linear # linear, mlp, add, none
      mode_init: xavier # uniform, xavier
      scale: 5.0
    latent_query_use_tf_decoder: False # True: (cross+self)*n, False: cross+self*n
    multi_modal_anchors:
      use_agent_type: True
      mode_emb: linear # linear, mlp, add, none
      mode_init: xavier # uniform, xavier
      scale: 5.0
    anchor_self_attn: True
    mlp_head:
      predictions: [pos, cov3, spd, vel, yaw_bbox] # keywords: pos, cov1/2/3, spd, vel, yaw_bbox
      use_agent_type: False
      flatten_conf_head: False
      out_mlp_layernorm: False
      out_mlp_batchnorm: False
      n_step_future: 11
    use_attr_for_multi_modal: False # use agent_attr instead of agent_emb for learnable latent/anchor
    use_vmap: True

# * optimizer
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-4 # 2e-4, 1e-5
lr_scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  gamma: 0.5
  step_size: 25

sub_womd:
  _target_: hptr_modules.utils.submission.SubWOMD
  activate: False
  method_name: METHOD_NAME
  authors: [NAME1, NAME2]
  affiliation: AFFILIATION
  description: scr_womd
  method_link: METHOD_LINK
  account_name: ACCOUNT_NAME
sub_av2:
  _target_: hptr_modules.utils.submission.SubAV2
  activate: False
